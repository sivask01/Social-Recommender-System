{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eocTO5CMMWRY",
    "outputId": "6d05e1c0-22e3-4157-af5f-aed002d85e8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4IegDTC1MZ5E",
    "outputId": "604efceb-eb51-4252-be16-289e335c207f"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "#from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the fine-tuned BERT model and tokenizer\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# English, Dutch, German, French, Spanish, and Italian\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Hugging Face pipeline for sentiment analysis with the device argument\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, batch_size=8, max_length=512, device=device, truncation=True)\n",
    "\n",
    "# Truncate text to fit the model's maximum token length\n",
    "def truncate_text(text, max_length=512):\n",
    "    # Adjust max length to account for special tokens\n",
    "    max_length -= tokenizer.num_special_tokens_to_add(pair=False)\n",
    "\n",
    "    # Tokenize and truncate\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) > max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "\n",
    "    # Reconstruct text from truncated tokens\n",
    "    return tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "\n",
    "# Function to get sentiment scores with logging every 10,000 rows\n",
    "def get_sentiment_scores_bert(df):\n",
    "    sentiment_scores = []\n",
    "    start_time = time.time()\n",
    "    for index, text in enumerate(df['clean_comment']):\n",
    "        # Truncate the text\n",
    "     #   truncated_text = truncate_text(text)\n",
    "\n",
    "        # Get the sentiment score\n",
    "        result = sentiment_pipeline(text)[0]\n",
    "        score = int(result['label'].split()[0])  # Convert \"5 stars\" -> 5\n",
    "\n",
    "        sentiment_scores.append(score)\n",
    "\n",
    "        # Log progress every 10,000 rows\n",
    "        if (index + 1) % 10000 == 0:\n",
    "            elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
    "            print(f\"Processed {index + 1} rows. Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return sentiment_scores\n",
    "\n",
    "file_path = '/content/drive/MyDrive/full_cleaned.csv'\n",
    "\n",
    "filter_df = pd.read_csv(file_path)\n",
    "filter_df.info()\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "filter_df['sentiment_score_bert'] = get_sentiment_scores_bert(filter_df)\n",
    "\n",
    "# Display the results\n",
    "print(filter_df)\n",
    "\n",
    "filter_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0oq2asrbY2PN",
    "outputId": "16772d93-66cb-4d73-a099-0c0af9e83600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         stars  sentiment_score_bert\n",
      "stars                 1.000000              0.514099\n",
      "sentiment_score_bert  0.514099              1.000000\n",
      "Pearson correlation between actual stars and BERT-generated stars: 0.5140989053352806\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_edges_reviews' is your DataFrame and it contains:\n",
    "# 'stars' - actual ratings (numeric)\n",
    "# 'sentiment_score_bert' - generated sentiment scores (numeric)\n",
    "\n",
    "# Calculate the correlation between 'stars' and 'sentiment_score_bert'\n",
    "correlation = filter_df[['stars', 'sentiment_score_bert']].corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation)\n",
    "\n",
    "# Alternatively, if you want just the Pearson correlation value between the two columns:\n",
    "pearson_corr = correlation.loc['stars', 'sentiment_score_bert']\n",
    "print(f\"Pearson correlation between actual stars and BERT-generated stars: {pearson_corr}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
